{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":8940774,"sourceType":"competition"},{"sourceId":3729,"sourceType":"modelInstanceVersion","modelInstanceId":2656},{"sourceId":72141,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":59770},{"sourceId":72166,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":59770}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- [Training notebook](https://www.kaggle.com/code/motono0223/isic-pytorch-training-baseline-image-only)\n- Inference notebook (This notebook)","metadata":{"_uuid":"359ea002-8c21-49d8-9482-b7ba773cb78e","_cell_guid":"8f694a48-6f58-4aec-baef-1552fc4fc4af","trusted":true}},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries üìö</h1></span>","metadata":{"_uuid":"c3381846-61ef-46c7-a56a-a2ded6be6577","_cell_guid":"03b3fb5e-423d-4adf-bf02-92549a2e67b7","trusted":true}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\n\nimport h5py\nfrom PIL import Image\nfrom io import BytesIO\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torchvision\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\n# For Image Models\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_uuid":"9a12a3e9-ad02-4fb5-8dbf-7b7f51a8b726","_cell_guid":"9170d5db-e149-4ab1-978e-6dc80ead080e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-07-04T11:03:23.884696Z","iopub.execute_input":"2024-07-04T11:03:23.885055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration ‚öôÔ∏è</h1></span>","metadata":{"_uuid":"c407167e-5013-41e7-a1b2-e0f39ac3338b","_cell_guid":"708f548b-24a2-4d9c-95c4-3ac5b7ef21ae","trusted":true}},{"cell_type":"code","source":"CONFIG = {\n    \"seed\": 42,\n    \"img_size\": 224,\n    \"model_name\": \"tf_efficientnet_b0_ns\",\n    \"valid_batch_size\": 32,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n}","metadata":{"_uuid":"3e1706c1-9117-4b81-acac-9599136f3fd5","_cell_guid":"facef6be-96d7-41d5-99f7-631c7bb92b07","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility</h1></span>","metadata":{"_uuid":"47555374-d4e9-4e04-9322-89cff7d9b4ff","_cell_guid":"4f81a42f-c985-478f-9bd7-0332f99048fb","trusted":true}},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","metadata":{"_uuid":"d0e224a1-d454-42b9-ad3b-e15ff600572b","_cell_guid":"9d81f133-022e-463e-a101-8460d3c37d92","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR = \"/kaggle/input/isic-2024-challenge\"\nTEST_CSV = f'{ROOT_DIR}/test-metadata.csv'\nTEST_HDF = f'{ROOT_DIR}/test-image.hdf5'\nSAMPLE = f'{ROOT_DIR}/sample_submission.csv'\n\nBEST_WEIGHT = \"/kaggle/input/isic-2024-training/ISIC2024/src/pAUC0.1615_Loss0.2771_epoch27.bin\"\nEFFICIENT_NET_EXTRA2018 = \"/kaggle/input/isic2024_models/pytorch/models/2/pAUC0.0771_Loss10.2190_epoch14.bin\"","metadata":{"_uuid":"189fd8b9-76c7-4b1f-8568-4ff261571717","_cell_guid":"1b421fff-44a9-454b-91b6-c49b96a4b9a5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data üìñ</h1>","metadata":{"_uuid":"a30a79a9-a9c9-4a2a-bbea-714b55b3ed1c","_cell_guid":"df255b1b-3058-4c94-911f-cd9e6a46b85a","trusted":true}},{"cell_type":"code","source":"df = pd.read_csv(TEST_CSV)\ndf['target'] = 0 # dummy\n\ndf_sub = pd.read_csv(SAMPLE)","metadata":{"_uuid":"cd0e7203-7152-40ea-b49f-5317669f82fa","_cell_guid":"d842224f-847a-486d-a29c-dadbb1aa743d","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class</h1></span>","metadata":{"_uuid":"a3bb3852-9ff0-4dd2-b523-1152b5576d96","_cell_guid":"96143163-9ab1-4623-900d-f9d69eacd6e4","trusted":true}},{"cell_type":"code","source":"class ISICDataset(Dataset):\n    def __init__(self, df, file_hdf, transforms=None, meta_feature_columns=None):\n        self.df = df\n        self.meta_feature_columns = meta_feature_columns\n        self.fp_hdf = h5py.File(file_hdf, mode=\"r\")\n        self.isic_ids = df['isic_id'].values\n        self.targets = df['target'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.isic_ids)\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        \n        isic_id = self.isic_ids[index]\n        img = np.array( Image.open(BytesIO(self.fp_hdf[isic_id][()])) )\n        target = self.targets[index]\n        \n        if self.transforms:\n            image = self.transforms(image=img)[\"image\"]\n        \n        if self.meta_feature_columns is not None:\n            # Load meta data and fill missing values\n            meta = row[self.meta_feature_columns].values.astype(np.float32)\n            meta = np.nan_to_num(meta)\n            meta = torch.tensor(meta, dtype=torch.float)\n            # print(\"[INFO] meta in TBP_Dataset:\", meta)\n            \n            return {\n                'image': image,\n                'target': target,\n                'meta': meta\n            }\n        else:\n            return {\n                'image': image,\n                'target': target\n            }","metadata":{"_uuid":"45955f19-bf14-4918-bd7f-205066568689","_cell_guid":"38a6cc9a-ba69-40e3-a46d-d103d0c266dc","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Augmentations</h1></span>","metadata":{"_uuid":"94bc0b71-e422-4a8b-82db-0cdfc0225421","_cell_guid":"42d8a5eb-2ef7-45f4-b1bc-bea88962ee53","trusted":true}},{"cell_type":"code","source":"data_transforms = {\n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","metadata":{"_uuid":"ba2c6adc-5da9-447e-95ed-2bed174aa0cb","_cell_guid":"1902f5c2-1345-48bc-8f85-888563027901","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">GeM Pooling</h1></span>","metadata":{"_uuid":"019ac5c0-558d-4c3c-9278-83b201b2c40c","_cell_guid":"343b2702-5a17-4cfe-a8d5-7c30e194dde4","trusted":true}},{"cell_type":"code","source":"class GeM(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super(GeM, self).__init__()\n        self.p = nn.Parameter(torch.ones(1)*p)\n        self.eps = eps\n\n    def forward(self, x):\n        return self.gem(x, p=self.p, eps=self.eps)\n        \n    def gem(self, x, p=3, eps=1e-6):\n        return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n        \n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n                ', ' + 'eps=' + str(self.eps) + ')'","metadata":{"_uuid":"0fb04b43-671d-481a-9d38-3a8ba440a4c5","_cell_guid":"c261f1d9-b8ca-433e-b0ed-a15d14aaf18e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model</h1></span>","metadata":{"_uuid":"7c71e5d1-36eb-4993-b62d-c3fc3432d8ab","_cell_guid":"4141596d-be40-446e-85ec-b099839e99bd","trusted":true}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch import optim\nimport torchvision\nfrom torchvision import models\n\nclass Swish(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i * nn.Sigmoid()(i)\n        ctx.save_for_backward(i)\n        return result\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n        sigmoid_i = nn.Sigmoid()(i)\n        return grad_output * (sigmoid_i * (1 + i * (1 - sigmoid_i)))\n\n\nclass Swish_Module(nn.Module):\n    def forward(self, x):\n        return Swish.apply(x)\n\nclass ImageBranch(nn.Module):\n    def __init__(self, model_name='efficientnet_b0', pretrained=False):\n        super(ImageBranch, self).__init__()\n        self.model_name = model_name\n        self.pretrained = pretrained\n        self.cnn = self._create_cnn_model()\n        self.output_dim = self._get_output_dim()\n        \n    def _create_cnn_model(self):\n        model_architectures = {\n            'resnet18': torchvision.models.resnet18,\n            'vgg16': torchvision.models.vgg16,\n            'efficientnet_b0': torchvision.models.efficientnet_b0,\n            'efficientnet_b1': torchvision.models.efficientnet_b1,\n            'efficientnet_b2': torchvision.models.efficientnet_b2,\n            'efficientnet_b3': torchvision.models.efficientnet_b3,\n            'efficientnet_b4': torchvision.models.efficientnet_b4,\n            'efficientnet_b5': torchvision.models.efficientnet_b5,\n            'efficientnet_b6': torchvision.models.efficientnet_b6,\n            'efficientnet_b7': torchvision.models.efficientnet_b7\n        }\n        \n        model = model_architectures[self.model_name](pretrained=self.pretrained)\n        \n        if self.model_name in ['resnet18', 'vgg16']:\n            model.fc = nn.Identity()\n            model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        elif self.model_name.startswith('efficientnet_b'):\n            model.classifier = nn.Identity()\n            model.avgpool = GeM()\n        else:\n            raise ValueError(f\"Unsupported model: {self.model_name}\\n Supported models: resnet18, vgg16, efficientnet_b 0 to 7\")\n        \n        return model\n\n    def _get_output_dim(self):\n        lookup = {\n            'resnet18': 512,\n            'vgg16': 4096,\n            'efficientnet_b0': 1280,\n            'efficientnet_b1': 1280,\n            'efficientnet_b2': 1408,\n            'efficientnet_b3': 1536,\n            'efficientnet_b4': 1792,\n            'efficientnet_b5': 2048,\n            'efficientnet_b6': 2304,\n            'efficientnet_b7': 2560\n        }\n        dim = lookup.get(self.model_name, None) \n        if dim is None:\n            raise ValueError(f\"Unsupported model: {self.model_name} \\n Supported models: resnet18, vgg16, efficientnet_b 0 to 7\")\n        \n        return dim\n\n    def forward(self, x):\n        x = self.cnn(x)\n        return x\n\nclass MetadataBranch(nn.Module):\n    def __init__(self, metadata_dim, hidden_dims=[512], output_dim=128):\n        super(MetadataBranch, self).__init__()\n        self.meta = nn.Sequential(\n            nn.Linear(metadata_dim, hidden_dims[0]),\n            nn.BatchNorm1d(hidden_dims[0]),\n            Swish_Module(),\n            nn.Dropout(p=0.5),\n            \n            nn.Linear(hidden_dims[0], output_dim),\n            nn.BatchNorm1d(output_dim),\n            Swish_Module(),\n        )\n    \n    def forward(self, x):\n        x = self.meta(x)\n        return x\n    \nclass CombinedModel(nn.Module):\n    def __init__(self, image_model_name, metadata_dim=0, hidden_dims=[512, 128], metadata_output_dim=128):\n        \"\"\"\n        Initializes the CombinedAttentionModel with the given hyperparameters.\n\n        Args:\n            image_model_name (str): The name of the image model.\n            metadata_dim (int, optional): The dimension of the metadata. Defaults to 0.\n            hidden_dims (list, optional): The hidden dimensions for the metadata branch. Defaults to [512, 128].\n            metadata_output_dim (int, optional): The output dimension for the metadata branch. Defaults to 128.\n        \"\"\"\n        super(CombinedModel, self).__init__()\n        \n        # Initialize hyperparameters\n        self.metadata_dim = metadata_dim\n        \n        self.image_branch = ImageBranch(model_name=image_model_name)\n        \n        # Calculate combined dimension\n        combined_dim = self.image_branch.output_dim \n        \n        # Initialize metadata branch if metadata_dim > 0\n        if metadata_dim > 0:\n            self.metadata_branch = MetadataBranch(metadata_dim=metadata_dim, hidden_dims=hidden_dims, output_dim=metadata_output_dim)\n            combined_dim += metadata_output_dim\n        \n        # Initialize final layer\n        self.fc = nn.Sequential(\n            nn.Dropout(p=0.5),  # Dropout layer\n            nn.Linear(combined_dim, 1),  # Hidden layer\n        )\n        \n        self.sigmoid = nn.Sigmoid()\n    \n    def forward(self, image, metadata):\n        \"\"\"\n        Forward pass of the combined attention model.\n\n        Args:\n            image (torch.Tensor): The input image tensor.\n            metadata (torch.Tensor): The input metadata tensor.\n\n        Returns:\n            torch.Tensor: The output tensor after passing through the model.\n        \"\"\"\n        # Pass image through image branch and attention\n        x = self.image_branch(image)\n        \n        # If metadata dimension is greater than zero, pass metadata through metadata branch and attention\n        if self.metadata_dim > 0:\n            x_meta = self.metadata_branch(metadata)\n            x = torch.cat([x, x_meta], dim=1)\n        \n        # Pass feature maps through final layer\n        output = self.sigmoid(self.fc(x))\n        \n        return output\n","metadata":{"_uuid":"2bbc97e7-cc1d-4e15-ba41-83a2efe29284","_cell_guid":"05349efb-33a0-49c6-b85f-5e4a45706978","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"_uuid":"c612bd32-13a4-4747-93bb-e7624303a4b1","_cell_guid":"2561bd7d-a14a-48f3-8fc2-e13304d44697","trusted":true}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import OrdinalEncoder\ndef feature_engineering(df):\n    \"\"\"\n    Performs feature engineering on the input DataFrame.\n\n    Args:\n        df (pandas.DataFrame): DataFrame containing the input data.\n\n    Returns:\n        pandas.DataFrame: DataFrame containing the processed data.\n        list: List of column names representing the meta-features.\n    \"\"\"\n    # Perform feature engineering\n    df[\"age_approx\"] = df[\"age_approx\"] / 100  # Normalize age\n    \n    # New features to try...\n    df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n    df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n    df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n    df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n    df[\"combined_anatomical_site\"] = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n    df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n    \n    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n    df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n    df[\"3d_lesion_orientation\"] = np.arctan2(df[\"tbp_lv_y\"], df[\"tbp_lv_x\"])\n    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n\n    # Define the meta-feature columns\n    new_num_cols = [\n        \"lesion_size_ratio\", \"lesion_shape_index\", \"hue_contrast\",\n        \"luminance_contrast\", \"lesion_color_difference\", \"border_complexity\",\n        \"color_uniformity\", \"3d_position_distance\", \"perimeter_to_area_ratio\",\n        \"lesion_visibility_score\", \"symmetry_border_consistency\", \"color_consistency\",\n\n        \"size_age_interaction\", \"hue_color_std_interaction\", \"lesion_severity_index\", \n        \"shape_complexity_index\", \"color_contrast_index\", \"log_lesion_area\",\n        \"normalized_lesion_size\", \"mean_hue_difference\", \"std_dev_contrast\",\n        \"color_shape_composite_index\", \"3d_lesion_orientation\", \"overall_color_difference\",\n        \"symmetry_perimeter_interaction\", \"comprehensive_lesion_index\",\n    ]\n    new_cat_cols = [\"combined_anatomical_site\"]\n    \n    num_cols = [\n        'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', \n        'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', \n        'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', \n        'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n        'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n        'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n        'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n        'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n        'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z',\n    ] + new_num_cols\n    \n    cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"] + new_cat_cols\n    \n    meta_feature_columns = num_cols + cat_cols\n    \n    category_encoder = OrdinalEncoder(\n        categories='auto',\n        dtype=int,\n        handle_unknown='use_encoded_value',\n        unknown_value=-2,\n        encoded_missing_value=-1,\n    )\n\n    X_cat = category_encoder.fit_transform(df[cat_cols])\n    for c, cat_col in enumerate(cat_cols):\n        df[cat_col] = X_cat[:, c]\n        \n    return df, meta_feature_columns","metadata":{"_uuid":"9f79ccd4-5dd2-48b9-9fbe-c43e5441bebd","_cell_guid":"8c0305d7-8b6a-426a-8805-2d8d7391dae0","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Engineering\ndf, meta_feature_columns = feature_engineering(df)\nckpt = \"/kaggle/input/isic2024_models/pytorch/models/7/CM_Extra_FE_Neg3_0.001decay_pAUC0.1703_Loss0.3327_epoch48.bin\"\n\n# ckpt = \"/kaggle/input/isic2024_models/pytorch/models/7/CM_Extra_NotFE_Neg3_0.001decay_224_pAUC0.1665_Loss0.3334_epoch41.bin\"\n# meta_feature_columns = None","metadata":{"_uuid":"a11df757-ec81-44c4-bca0-ee3b9fb52fb7","_cell_guid":"6b82bdd1-3ee5-474f-952a-84ab99b75705","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare data\ntest_dataset = ISICDataset(df, \n                           TEST_HDF, \n                           transforms=data_transforms[\"valid\"], \n                           meta_feature_columns=meta_feature_columns)\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n                          num_workers=2, shuffle=False, pin_memory=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = [\"efficientnet_b0\", \"efficientnet_b0\"]\nckpts = [\n    {\n        \"ckpt\": \"/kaggle/input/isic2024_models/pytorch/models/7/CM_Extra_FE_Neg3_0.001decay_pAUC0.1703_Loss0.3327_epoch48.bin\", \n        \"fe\" : True\n    },\n    {\n        \"ckpt\": \"/kaggle/input/isic2024_models/pytorch/models/7/CM_Extra_NotFE_Neg3_0.001decay_224_pAUC0.1665_Loss0.3334_epoch41.bin\",\n        \"fe\" : False\n    }\n]\nmodels = []\nfor i, model_name in enumerate(model_names):\n    model = CombinedModel(image_model_name = model_name, \n               metadata_dim=len(meta_feature_columns) if ckpts[i][\"fe\"] else 0, \n               hidden_dims=[512, 128], \n               metadata_output_dim=128)\n    model.load_state_dict( torch.load(ckpts[i][\"ckpt\"], map_location=torch.device('cpu')) )\n    model.to(CONFIG['device'])\n    model.eval()\n    models.append(model)","metadata":{"_uuid":"3fd05e36-0b80-4f68-9a59-23bcd4477735","_cell_guid":"de41e8b1-95a7-4966-86b1-7b34f1800cb5","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Start Inference</span>","metadata":{"_uuid":"a09cdac9-a5b5-43fb-affc-e2eca84ef041","_cell_guid":"91d5df4f-05be-49ca-a852-42f78d059187","trusted":true}},{"cell_type":"code","source":"for i, model in enumerate(models):\n    preds = []\n    with torch.no_grad():\n        bar = tqdm(enumerate(test_loader), total=len(test_loader))\n        for step, data in bar:        \n            images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)\n            if meta_feature_columns is not None:\n                meta = data['meta'].to(CONFIG[\"device\"], dtype=torch.float)\n            else:\n                meta = None\n            batch_size = images.size(0)\n            outputs = model(images, meta)\n            preds.append( outputs.detach().cpu().numpy() )\n    preds = np.concatenate(preds).flatten()\n    preds = np.nan_to_num(preds)\n    preds = np.clip(preds, 0, 1)\n    \n    df_sub[\"target\"] = preds\n    df_sub.to_csv(f\"submission_{i}.csv\", index=False)\n","metadata":{"_uuid":"c8787209-2c65-499f-a873-c971cd911ee0","_cell_guid":"55f5bdde-03e5-4719-a90e-c14fe3eca05e","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"number_of_models = len(model_names)\nsubs = []\nfor i in range(number_of_models):\n    subs.append(pd.read_csv(f\"submission_{i}.csv\"))\n\n# Extract the target values\nsub_probs = [sub.target.values for sub in subs]\n\n# Weights for the models (assuming equal weighting)\nwts = [1/number_of_models] * number_of_models\n\nassert len(wts) == len(sub_probs)\n\n# Weighted sum of raw target values\nsub_ens = np.sum([wts[i] * sub_probs[i] for i in range(len(wts))], axis=0)\n\n# Create final submission DataFrame\ndf_sub = subs[0]\ndf_sub['target'] = sub_ens\ndf_sub.to_csv(f\"submission.csv\", index=False)\n\ndf_sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}